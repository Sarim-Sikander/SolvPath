{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e394334d",
   "metadata": {
    "id": "e394334d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection  import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder,LabelEncoder\n",
    "from collections import Counter\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, GRU,SimpleRNN\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.utils import np_utils,pad_sequences\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "oneE = OneHotEncoder(sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3018fee",
   "metadata": {
    "id": "a3018fee"
   },
   "outputs": [],
   "source": [
    "sentiment = pd.read_csv('Labeled_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490c77f8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "490c77f8",
    "outputId": "983dcaed-8bf3-4c39-ca10-d375f097ac9b"
   },
   "outputs": [],
   "source": [
    "sentiment['sentiment'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa45e6da",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fa45e6da",
    "outputId": "691eca7f-260d-462f-9ce9-f7dda43fb47e"
   },
   "outputs": [],
   "source": [
    "sentiment['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6b853f",
   "metadata": {
    "id": "ec6b853f"
   },
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9b9d2f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7f9b9d2f",
    "outputId": "6f202315-b0f1-4ccd-e53a-c32f0c278dec"
   },
   "outputs": [],
   "source": [
    "stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"])\n",
    "\n",
    "def preprocess(data):\n",
    "    preprocessed_reviews = []\n",
    "    for sentance in tqdm(data):\n",
    "        sentance = re.sub(r\"http\\S+\", \"\", sentance)\n",
    "        sentance = BeautifulSoup(sentance, 'lxml').get_text()\n",
    "        sentance = decontracted(sentance)\n",
    "        sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
    "        sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n",
    "        sentance = ' '.join([word for word in sentance.split() if len(word) > 1])\n",
    "        sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords)\n",
    "        preprocessed_reviews.append(sentance.strip())\n",
    "    return preprocessed_reviews\n",
    "sentiment_text = preprocess(sentiment['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc2bfcd",
   "metadata": {
    "id": "7bc2bfcd"
   },
   "outputs": [],
   "source": [
    "def sent2Words(preprocess_text):\n",
    "    i=0\n",
    "    list_of_sentance=[]\n",
    "    for sentance in preprocess_text:\n",
    "        list_of_sentance.append(sentance.split())\n",
    "    return list_of_sentance\n",
    "sentiment_sentence = sent2Words(sentiment_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb1e248",
   "metadata": {
    "id": "9cb1e248"
   },
   "source": [
    "# SENTIMENT DATASET TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15117d8f",
   "metadata": {
    "id": "15117d8f"
   },
   "outputs": [],
   "source": [
    "X=sentiment_text\n",
    "y=sentiment['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-xW58urXozmn",
   "metadata": {
    "id": "-xW58urXozmn"
   },
   "outputs": [],
   "source": [
    "X = X[:-1]\n",
    "y = y[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acf7664",
   "metadata": {
    "id": "5acf7664"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "def tokenize(data,max_features=20000):\n",
    "    max_fatures=max_features\n",
    "    tokenizer = Tokenizer(num_words=max_fatures)\n",
    "    tokenizer.fit_on_texts(data)\n",
    "    dictionary = tokenizer.word_index\n",
    "    with open('wordindex.json', 'w') as dictionary_file:\n",
    "        json.dump(dictionary, dictionary_file) \n",
    "    return tokenizer,dictionary,max_fatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c2fbcf",
   "metadata": {
    "id": "43c2fbcf"
   },
   "outputs": [],
   "source": [
    "tokenizer,dictionary,max_features = tokenize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2984bd1",
   "metadata": {
    "id": "c2984bd1"
   },
   "outputs": [],
   "source": [
    "def text_to_seq(tokenizer,CodeSnippet,seq=100):\n",
    "    X = tokenizer.texts_to_sequences(CodeSnippet)\n",
    "    X = pad_sequences(X,seq)    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51484a63",
   "metadata": {
    "id": "51484a63"
   },
   "outputs": [],
   "source": [
    "X = text_to_seq(tokenizer,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c19d9a",
   "metadata": {
    "id": "07c19d9a"
   },
   "outputs": [],
   "source": [
    "def one_hot_encoder(y,inverse=False):\n",
    "    if inverse:\n",
    "        Y = oneE.inverse_transform(y)\n",
    "    else:\n",
    "        y = np.array(y)\n",
    "        y = y.reshape(-1,1)\n",
    "        Y = oneE.fit_transform(y)\n",
    "    return Y,oneE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972f1383",
   "metadata": {
    "id": "972f1383"
   },
   "outputs": [],
   "source": [
    "Y,oneE = one_hot_encoder(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rQ8GoyRZsnud",
   "metadata": {
    "id": "rQ8GoyRZsnud"
   },
   "source": [
    "# **SPLITTING DATASETS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935b8b26",
   "metadata": {
    "id": "935b8b26"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "od8gdp8Rtqvp",
   "metadata": {
    "id": "od8gdp8Rtqvp"
   },
   "source": [
    "## DEEP LEARNING MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j0WkhaBgpX6M",
   "metadata": {
    "id": "j0WkhaBgpX6M"
   },
   "outputs": [],
   "source": [
    "# SIMPLE RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0105337",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d0105337",
    "outputId": "cc483996-a202-4288-a3e1-c78ab28f92fe"
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "max_len = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                 200,\n",
    "                 input_length=max_len))\n",
    "model.add(SimpleRNN(100))\n",
    "model.add(Dense(Y.shape[1], activation='sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3750d718",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3750d718",
    "outputId": "b3f5a39c-ad7c-4716-9407-531102be12dc"
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train,validation_data=(X_test,y_test),epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10073d1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d10073d1",
    "outputId": "50f99161-b1fd-4e9d-b887-9d048b488319"
   },
   "outputs": [],
   "source": [
    "scores = model.predict(X_test)\n",
    "one_hot_encoder(scores,inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3JoaoWq_-5VZ",
   "metadata": {
    "id": "3JoaoWq_-5VZ"
   },
   "outputs": [],
   "source": [
    "model.save('SimpleRNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mNOqHNSJphci",
   "metadata": {
    "id": "mNOqHNSJphci"
   },
   "outputs": [],
   "source": [
    "# LSTM WITH WORD EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ec8262",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d9ec8262",
    "outputId": "73cc9c30-87be-47d9-d302-3216f6acae39"
   },
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open('/content/drive/MyDrive/glove.840B.300d.txt','r',encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.split(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray([float(val) for val in values[1:]])\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609591e1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "609591e1",
    "outputId": "f3958b1b-ba9b-477f-878b-60c0c4d0f323"
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728270ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "728270ec",
    "outputId": "9f252626-7a22-4c3a-e1b4-6a28509c5346"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                 300,\n",
    "                 weights=[embedding_matrix],\n",
    "                 input_length=max_len,\n",
    "                 trainable=False))\n",
    "\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "model.add(Dense(Y.shape[1], activation='sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1536de",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b1536de",
    "outputId": "b8ba3fcf-63f6-4fdc-8a75-5d2aa6a8a598"
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train,validation_data=(X_test,y_test),epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13edbd62",
   "metadata": {
    "id": "13edbd62"
   },
   "outputs": [],
   "source": [
    "model.save('models/LSTM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5BPz4qAipnou",
   "metadata": {
    "id": "5BPz4qAipnou"
   },
   "outputs": [],
   "source": [
    "# LSTM + GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e8c9b4",
   "metadata": {
    "id": "d6e8c9b4",
    "outputId": "c9b49fd0-b81c-483c-8564-19588eea2a9d"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "             300,\n",
    "             weights=[embedding_matrix],\n",
    "             input_length=max_len,\n",
    "             trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(GRU(300))\n",
    "model.add(Dense(Y.shape[1], activation='sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaba8925",
   "metadata": {
    "id": "aaba8925",
    "outputId": "81140530-74ec-4db6-cd9d-00ffecba9a62"
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train,validation_data=(X_test,y_test),epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "S4AnZCswpsF5",
   "metadata": {
    "id": "S4AnZCswpsF5"
   },
   "outputs": [],
   "source": [
    "# BIDIRECTIONAL LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d336805b",
   "metadata": {
    "id": "d336805b"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                 300,\n",
    "                 weights=[embedding_matrix],\n",
    "                 input_length=max_len,\n",
    "                 trainable=False))\n",
    "model.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n",
    "model.add(Dense(Y.shape[1], activation='sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c18f8ea",
   "metadata": {
    "id": "5c18f8ea"
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train,validation_data=(X_test,y_test),epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9748020f",
   "metadata": {
    "id": "ju1ynC2_-Kte"
   },
   "source": [
    "# BERT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170abafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=sentiment_text\n",
    "y=sentiment['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071db568",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9973f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1864ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment['text'].apply(lambda x: len(str(x))).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f70c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 256\n",
    "batch_size = 32\n",
    "num_samples = len(X_train)\n",
    "model_name = 'distilbert-base-uncased-finetuned-sst-2-english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576f465e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_tokens = tokenizer(X_train, max_length=seq_len, \n",
    "                         truncation=True, padding='max_length', \n",
    "                         add_special_tokens=True, return_tensors='np')\n",
    "\n",
    "# y_train = ytrain['target'].values\n",
    "labels = np.zeros((num_samples, y_train.max() + 1))\n",
    "labels[np.arange(num_samples), y_train] = 1\n",
    "# labels = y_train\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_tokens['input_ids'], train_tokens['attention_mask'], labels))\n",
    "\n",
    "def map_func(input_ids, masks, labels):\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': masks\n",
    "    }, labels\n",
    "\n",
    "dataset = dataset.map(map_func)\n",
    "dataset = dataset.shuffle(10000).batch(batch_size=batch_size, drop_remainder=True)\n",
    "\n",
    "split = 0.9\n",
    "size = int((train_tokens['input_ids'].shape[0] // batch_size) * split)\n",
    "\n",
    "train_ds = dataset.take(size)\n",
    "val_ds = dataset.skip(size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d84b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFAutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Two inputs\n",
    "input_ids = tf.keras.layers.Input(shape=(seq_len,), name='input_ids', dtype='int32')\n",
    "mask = tf.keras.layers.Input(shape=(seq_len,), name='attention_mask', dtype='int32')\n",
    "\n",
    "# Transformer\n",
    "# embeddings = model.bert(input_ids, attention_mask=mask)[1]\n",
    "embeddings = model(input_ids, attention_mask=mask)[0]\n",
    "embeddings = embeddings[:, 0, :]\n",
    "# Classifier head\n",
    "x = tf.keras.layers.Dense(512, activation='relu')(embeddings)\n",
    "# x = tf.keras.layers.Dropout(0.1)(x)\n",
    "y = tf.keras.layers.Dense(len(set(y)), activation='softmax', name='outputs')(x)\n",
    "\n",
    "bert_model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
    "\n",
    "# freeze bert layers\n",
    "bert_model.layers[2].trainable = False\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=1e-5)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "bert_model.compile(optimizer=optimizer, loss=loss, metrics=[acc])\n",
    "\n",
    "history = bert_model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=5,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d28c94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Models.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
